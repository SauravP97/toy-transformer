{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495304e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-01 13:12:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.11MB/s    in 0.5s    \n",
      "\n",
      "2026-01-01 13:12:24 (2.11 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8181ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ea8567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f'length of text: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87b2fc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z\n",
      "vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(' '.join(chars))\n",
    "print(f'vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e85335",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s: [stoi[char] for char in s]\n",
    "decode = lambda l: ''.join([itos[index] for index in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "470e6a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "print(encode('hii there'))\n",
    "print(decode(encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d27e22c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravprateek/Documents/saurav-codes/neural-nets-codelab/saurav-env/lib/python3.13/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83da65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4db154f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridge: 1003854\n",
      "Train size: 1003854, Val size: 111540\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(f'bridge: {n}')\n",
    "print(f'Train size: {len(train_data)}, Val size: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c10973ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c2dff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: xb: torch.Size([4, 8]) and yb: torch.Size([4, 8])\n",
      "xb: tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "yb: tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(f'batch shape: xb: {xb.shape} and yb: {yb.shape}')\n",
    "print(f'xb: {xb}')\n",
    "print(f'yb: {yb}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed570279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    " \n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Head of self attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B,T,h)\n",
    "q = query(x) # (B,T,h)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, h) @ (B, h, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "\n",
    "# allow tokens to only talk to the tokens at previous positions.\n",
    "# avoid tokens to talk to the future tokens!\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) # B,T,h\n",
    "out = wei @ v\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c842cc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fab455ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "block_size = 32\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "# Embedding Dimension\n",
    "n_embed = 64\n",
    "# Number of Heads per Attention Block\n",
    "n_head = 4\n",
    "# Number of layers of Attention Blocks\n",
    "n_layer = 4\n",
    "dropout = 0\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # B,T,head_size\n",
    "        q = self.query(x) # B,T,head_size\n",
    "\n",
    "        # perform the weighted aggregation of the values \n",
    "        v = self.value(x) # B,T,head_size\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B,T,head_size) @ (B,head_size, T)  -> (B,T,T)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        # allow tokens to only talk to the tokens at previous positions.\n",
    "        # avoid tokens to talk to the future tokens!\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # B,T,T\n",
    "        wei = F.softmax(wei, dim=-1) # B,T,T\n",
    "\n",
    "        out = wei @ v # (B,T,T) @ (B,T,head_size) -> (B,T,head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "433c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff4da2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a simple linear ayer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16f901a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Residual Connections\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe25d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, n_head) for _ in range(n_layer)])\n",
    "        self.layer_norm = nn.LayerNorm(n_embed)\n",
    "        self.linear = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        token_embedding = self.token_embedding_table(idx) # B, T, n_embed\n",
    "        positional_embedding = self.positional_embedding_table(torch.arange(T, device=device)) # T, n_embed\n",
    "        x = token_embedding + positional_embedding # B, T, n_embed\n",
    "\n",
    "        x = self.blocks(x) # B, T, n_embed\n",
    "        x = self.layer_norm(x) # B, T, n_embed\n",
    "        logits = self.linear(x) # B, T, vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=1000):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_trimmed = idx[:, -block_size:] # B, T\n",
    "            logits, loss = self.forward(idx_trimmed) # B, T, n_embed\n",
    "            next_predicted_logit = logits[:,-1,:] # B, n_embed\n",
    "            probs = F.softmax(next_predicted_logit, dim=-1) # B, 1\n",
    "            next_predicted_character = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_predicted_character), dim=1) # B,T+1\n",
    "\n",
    "        return idx\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a04d2e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (token_embedding_table): Embedding(65, 64)\n",
       "  (positional_embedding_table): Embedding(32, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=64, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0260f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729  M parameters\n"
     ]
    }
   ],
   "source": [
    "print(sum(parameter.numel() for parameter in model.parameters()) / 1e6, ' M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7b273a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500 | Loss: 1.5014781765937806\n",
      "Iteration: 1000 | Loss: 1.5010536304712296\n",
      "Iteration: 1500 | Loss: 1.499786115805308\n",
      "Iteration: 2000 | Loss: 1.498626964211464\n",
      "Iteration: 2500 | Loss: 1.4969839842796326\n",
      "Iteration: 3000 | Loss: 1.495960422317187\n",
      "Iteration: 3500 | Loss: 1.4942270474774497\n",
      "Iteration: 4000 | Loss: 1.49324335116148\n",
      "Iteration: 4500 | Loss: 1.4922548153665331\n",
      "Iteration: 4999 | Loss: 1.4909873292264426\n"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses_per_eval_interval = []\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_interval == 0 or iter == (max_iters - 1):\n",
    "        if losses_per_eval_interval:\n",
    "            average_loss_per_eval_interval = sum(losses_per_eval_interval) / len(losses_per_eval_interval)\n",
    "            print(f'Iteration: {iter} | Loss: {average_loss_per_eval_interval}')\n",
    "    \n",
    "    Xb, Yb = get_batch('train')\n",
    "\n",
    "    logits, loss = model(Xb, Yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    losses_per_eval_interval.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5cd5ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "That mean in home, shoo prince. Let me, the to blame,\n",
      "That see spicial that be suns. Say he lady man's wellder\n",
      "I spake to looks on, shorthy,\n",
      "Let him or honour's furge. Lord His shally\n",
      "Purou, Lady York you good sees;\n",
      "He deaden that, a Capullius.\n",
      "\n",
      "CORIOLANUS:\n",
      "This new, thou mounths may with; Camille of any lady,\n",
      "And for thyself recore, sirn for York, a sequits?\n",
      "She with put haur is laboured sinful liber,\n",
      "With please that leak that was I knowful please.\n",
      "We musted perfear'd fings, if Richard yet he, the wail and to-day.\n",
      "\n",
      "Second Citizen:\n",
      "A bad horsome\n",
      "he iell? death for yet monums before ent:\n",
      "For that a heaph'n I have thee but it me;\n",
      "He, not tune again\n",
      "Thou:\n",
      "Let me not vallasterity, a means,\n",
      "And he to bark, an yet no. Wert Well he lay to which the eyes,\n",
      "Injoy'd all were be thee comes in my empate,\n",
      "And her, any looks that make the clows be of thoses;\n",
      "We way, good myst any, but a buringler\n",
      "And forth thou reason edger father exseen\n",
      "And ragal:\n",
      "I let not buy, if thou is shalt cannot len exprattly\n",
      "That's given you, bright proteen thee.\n",
      "\n",
      "WARWICK:\n",
      "Ay, what there? show you had for thee,\n",
      "You so I dear your please in her one that the loves.\n",
      "\n",
      "MARCIUS:\n",
      "I tell it beat hath a gallet hour that thou I\n",
      "sit know the chat bettally?\n",
      "\n",
      "SICINIUS:\n",
      "I is some sword,\n",
      "I'll makes a hopedes of ! how mock,\n",
      "O good homel'd by thy lay, but now.\n",
      "\n",
      "ANGELO:\n",
      "Mark you many of this convalours music boy time\n",
      "For breaked life. Edward, no make me to there?\n",
      "\n",
      "TRANIO:\n",
      "Reveren death 'twhing but now slew there.\n",
      "\n",
      "LEONTES:\n",
      "When you shall unto say not rick friends?\n",
      "\n",
      "Thirder:\n",
      "Unpear accepted, go promio in, and choose\n",
      "Prichaste to him leann a man\n",
      "And I'll dear hasting noble first angried!\n",
      "And take watch once noble her speaks;\n",
      "Thou hast were may in the put give commissing,\n",
      "As tyreen nothing mother's what well,\n",
      "And throw, for you, slaret, the feather Rosses. To take guest two ho modnessay from the Yate that\n",
      "takes, which all the breathed-handly\n",
      "Sheath!\n",
      "May shutory, and conveying, my if profery\n",
      "Made, ooth as one orlyt make nothi\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
